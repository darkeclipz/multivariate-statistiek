{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-boek is te vinden: ftp://ftp.tuebingen.mpg.de/pub/kyb/bresciani/Crawley%20-%20The%20R%20Book.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code voor deze week:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "TW2 MWS Week 2\n",
    "\n",
    "Variantie van de storingsterm\n",
    "\n",
    "ref: \n",
    "[1] Arie Buijs (2012). \"Statistiek om mee te werken\". 13.3.1 Variantie van de storingsterm.\n",
    "\n",
    "\n",
    "R-code\n",
    "\n",
    "1. Met de hand:\n",
    "\n",
    "# data invoeren\n",
    "X<-c(12,2,3,5,10,9,8)\n",
    "Y<-c(125,30,43,62,108,102,90)\n",
    "n<-length(X)\n",
    "one<-rep(1,7)\n",
    "# matrix vergelijking\n",
    "A<-cbind(X,one)\n",
    "b<-Y\n",
    "# laden library met matrixinverse\n",
    "library(MASS)\n",
    "# oplossing geeft vector met coefficienten a en b\n",
    "oplossing<-(ginv(t(A) %*% A) %*% t(A) %*% b)\n",
    "# storingsterm (fout) bepalen\n",
    "error<-A %*% oplossing-b\n",
    "# deze foutenvector geeft per datapuntje het verschil met de best passende lijn\n",
    "# de variantie van de stroingsterm wordt geschat door de grootheid S[error]^2 (SE2), formule zie Buijs [1] p.382.\n",
    "SE2<-(t(error) %*% error)/(n-2)\n",
    "STDV2=sqrt(SE2)\n",
    "# Q: wat is de interpretatie van dit getal?\n",
    "\n",
    "2. Quick and dirty:\n",
    "# data invoeren\n",
    "X<-c(12,2,3,5,10,9,8)\n",
    "Y<-c(125,30,43,62,108,102,90)\n",
    "# kleinste kwadraten methode laten uivoeren door R:\n",
    "lm(Y~X)\n",
    "# Q: Wat is de betekenis van deze twee getallen?\n",
    "# volgende stap is het bepalen van de fout van de benadering:\n",
    "summary(lm(Y~X))\n",
    "# Let vooral op de Residual standard error!\n",
    "# Wiskunde is leuk en educatief\n",
    "\n",
    "+++++++++++++++++++++\n",
    "\n",
    "TW2\n",
    "\n",
    "Schatten en voorspellen\n",
    "\n",
    "reference: \n",
    "[1] Arie Buijs (2012). \"Statistiek om mee te werken\". 13.3.2. Twee intervallen.\n",
    "\n",
    "R-code\n",
    "\n",
    "1. By hand:\n",
    "\n",
    "X<-matrix(c(52,63,78,101,90,104,72))\n",
    "Y<-matrix(c(250,305,358,449,401,466,340))\n",
    "n<-length(X)\n",
    "#famous five:\n",
    "sum(X)\n",
    "sum(X^2)\t#SSX\n",
    "sum(Y)\n",
    "sum(Y^2)\t#SSY\n",
    "sum(X*Y)\t#SSXY\n",
    "# toewijzen mighty five aan variabelen\n",
    "SSX<-sum(X^2)\n",
    "SSY<-sum(Y^2)\n",
    "SSXY<-sum(X*Y)\n",
    "# richtingscoefficient bepalen: \n",
    "Sxis<-sum((X-mean(X))^2)\n",
    "Sxiyi<-sum((X-mean(X))*(Y-mean(Y)))\n",
    "rc<-Sxiyi/Sxis\n",
    "# startgetal (intercept) bepalen\n",
    "intercept<-mean(Y)-rc*mean(X)\n",
    "# Q: wat is de vergelijking van de best passende lijn?\n",
    "# voorspelling bij gegeven waarde\n",
    "x=70\n",
    "y<-intercept+rc*x\n",
    "# variantie van de storingsterm\n",
    "varSe<-(sum(Y^2)-intercept*sum(Y)-rc*t(X)%*%Y)/(n-2)\n",
    "varSe\n",
    "stdvSe<-sqrt(varSe)\n",
    "stdvSe\n",
    "# bepalen van 95% betrouwbaarheidsinterval rond gegeven waarde van x\n",
    "# stap 1: bepaal de standaardafwijking voor gegeven x\n",
    "stdvx<-stdvSe*sqrt(1/n+(x-mean(X))^2/sum((X-mean(X))^2))\n",
    "# stap 2: bepaal t-waarde\n",
    "t<-qt(c(.975),df=5)\n",
    "# stap 3: bepaal betrouwbaarheidsinterval\n",
    "lower<-y-t*stdvx\n",
    "upper<-y+t*stdvx\n",
    "\n",
    "2. Quick and dirty\n",
    "X<-matrix(c(52,63,78,101,90,104,72))\n",
    "Y<-matrix(c(250,305,358,449,401,466,340))\n",
    "# regressielijn\n",
    "lm(Y~X)\n",
    "# schatting variantie storingsterm\n",
    "summary(lm(Y~X))\n",
    "# Q: Hoe groot is de variantie?\n",
    "# t.b.d.\n",
    "\n",
    "+++++++++++++++++++++\n",
    "\n",
    "TW2\n",
    "\n",
    "MVS\n",
    "\n",
    "Reference:\n",
    "[1] Michael J. Crawley (2013). \"The R-book\". Chapter 10 Regression.\n",
    "[2] Arie Buijs (2012). \"Statistiek om mee te werken\". 13.3 Schatten en voorspellen met de regressielijn\n",
    "\n",
    "\n",
    "R-code\n",
    "\n",
    "# 10.1 Linear regression\n",
    "\n",
    "# randomized data\n",
    "# X<-sort(rnorm(20,10,3))\n",
    "# Y<-sort(rnorm(20,50,6))\n",
    "# data tannin (X) & growth (Y) from the R-book\n",
    "X<-matrix(c(0,1,2,3,4,5,6,7,8))\n",
    "Y<-matrix(c(12,10,8,11,6,7,2,3,3))\n",
    "plot(X,Y)\n",
    "# find an objective method to determine the best estimates of the parameters\n",
    "# X = explanatory var; Y = response var; a = intercept; b = slope.\n",
    "# convenient method is Maximum Likelihood Estimates (MLE) --> Least Squares Method (LSM).\n",
    "# Assumptions: \n",
    "# ASU1: var(y) is constant\n",
    "# ASU2: explanatory var X without error\n",
    "# ASU3: residuals are measured on the scale of Y\n",
    "# ASU4: residuals are normally distributed. \n",
    "# now run the LSM to find the best fit\n",
    "model<-lm(Y~X)\n",
    "# richtingscoefficient bepalen: \n",
    "Sxis<-sum((X-mean(X))^2)\n",
    "Sxiyi<-sum((X-mean(X))*(Y-mean(Y)))\n",
    "rc<-Sxiyi/Sxis\n",
    "# startgetal (intercept) bepalen\n",
    "intercept<-mean(Y)-rc*mean(X)\n",
    "# plot the best fit and residuals\n",
    "abline(model,col=\"red\")\n",
    "yhat<-predict(model,X=X)\n",
    "# plot residuals\n",
    "join<-function(i)\n",
    "\tlines(c(X[i],X[i]),c(Y[i],yhat[i]),col=\"green\")\n",
    "sapply(1:20,join)\n",
    "# yhat predicted values for Y\n",
    "# now see how we find the minumium error.\n",
    "bs<-seq(rc-0.5,rc+0.5,0.01)\n",
    "SSE<-function(i) sum((Y-intercept-bs[i]*X)^2)\n",
    "plot(bs,sapply(1:length(bs),SSE),type=\"l\",ylim=c(25,160),\n",
    "\txlab=\"slobe b\",ylab=\"sum of squared residuals\",col=\"blue\")\n",
    "# adjust the ylim(..) accordingly. \n",
    "# Suggestion is to calculate sum of squares and use interval around it\n",
    "\n",
    "+++++++++++++++++++++\n",
    "\n",
    "#10.1.1 The famous five in R\n",
    "\n",
    "sum(X);sum(X^2);sum(Y);sum(Y^2);sum(X*Y);\n",
    "# or with matrix multiplication\n",
    "XY<-cbind(1,X,Y)\n",
    "t(XY) %*% XY\n",
    "# Q: what is the interpretation of these numbers?\n",
    "\n",
    "+++++++++++++++++++++\n",
    "\n",
    "#10.1.2 Corrected sums (..)\n",
    "\n",
    "SSX=sum((X-mean(X))^2)\n",
    "SSY=sum((Y-mean(Y))^2)\n",
    "SSXY=sum((X-mean(X))*(Y-mean(Y)))\n",
    "rc=SSXY/SSX\n",
    "intercept=mean(Y)-rc*mean(X)\n",
    "#check the results with LSM\n",
    "lm(Y~X)\n",
    "\n",
    "+++++++++++++++++++++\n",
    "\n",
    "#10.1.3 Degree of scatter\n",
    "\n",
    "# issue: two datasets with same intercept and slope can look quite different \n",
    "# aim is to quantify the degree of fit --> sum of squares of the residuals\n",
    "# better known as the error sum of squares SSE\n",
    "# SSE=sigma(y-a-bx)^2\n",
    "# be aware the \"lack of scatter\" is quantified with r, zero scatter giver r=1\n",
    "# SSY refers to total variation in the response variable\n",
    "# SSE is the unexplained variation in the response variable (Y)\n",
    "# SSY-SSE is the explained variation\n",
    "# r^2=(SSY-SSE)/SSE\n",
    "# sum of squares of the residuals:\n",
    "deviance(lm(Y~X))\n",
    "\n",
    "+++++++++++++++++++++\n",
    "\n",
    "#10.1.4 Analysis of variance in regression\n",
    "\n",
    "# SSR is the variation explained by the model or Regression Sum of Squares (SSR)\n",
    "SSR<-SSXY^2/SSX\n",
    "# SSE is the unexplained variation called the error sum of squares (SSE)\n",
    "SSE<-SSY-SSR\n",
    "# another way to calculate is:\n",
    "SSE1<-deviance(lm(Y~X))\n",
    "SSY1<-deviance(lm(Y~1))\n",
    "SSR1<-SSY1-SSE1\n",
    "# SSY df = n-1 because one parameter (yhat=mean(Y)) has to be estimated \n",
    "# SSE df = n-2 because has two estimated parameters, intercept and slope \n",
    "anova(lm(Y~X))\n",
    "# how to assess the F-ratio?\n",
    "# method 1\n",
    "# F-test, 1 df numerator and 7 df denumerator\n",
    "# uncertainty: alpha=0.05 --> certainty=0.95 and calculate the critical value (cv)\n",
    "cv<-qf(0.95,1,8)\n",
    "# F-value>>critical value --> reject H0\n",
    "# method 2\n",
    "# what is the probability of getting the calculated F-value (147.24)?\n",
    "1-pf(147.24,1,8)\n",
    "# p<0.001 so very unlikely indeed\n",
    "# p is the of obtaining a F-value greater than our calculated given H0 is true\n",
    "# Q: so H0 ..?\n",
    "  \n",
    "+++++++++++++++++++++\n",
    "\n",
    "#10.1.5 Unreliability estimates for the parameters\n",
    "\n",
    "# finding the slope and intercept is only hals the story\n",
    "# we need to measure the unreliability associated with each of the estimated parameters\n",
    "# so calculate the standard error of the slope and intercept\n",
    "# Standard error of the estimated slope\n",
    "# se[b]=sqrt(s^2/SSX)\n",
    "# uncertainty will increase with increasing variance and when range of x values is small, measure by SSX \n",
    "# uncertainty declines with increasing number of points\n",
    "# lets start with calculating the variance s^2, see Buijs 13.3.1, ref[2], n-2=7\n",
    "var<-(sum(Y^2)-intercept*sum(Y)-rc*sum(X*Y))/7\n",
    "# standard error of the slope (b)\n",
    "seb<-sqrt(var/SSX)\n",
    "# standard error of the intercept (a)\n",
    "sea<-sqrt(var*sum(X^2)/(9*SSX))\n",
    "# errors Quick And Dirty (QAD)\n",
    "summary(lm(Y~X))\n",
    "# find the errors sea & seb\n",
    "\n",
    "+++++++++++++++++++++\n",
    "\n",
    "#10.1.6 Prediction using the fitted model\n",
    "\n",
    "# good practice: safe results in named project\n",
    "model<-lm(Y~X)\n",
    "# predicting values\n",
    "predict(model,list(Y=5.5))\n",
    "\n",
    "+++++++++++++++++++++\n",
    "\n",
    "#10.1.7 Model checking\n",
    "\n",
    "# final thing: expose model to critical appraisal\n",
    "# be sure about consistancy of variance and normality of errors\n",
    "# how? model-checking plots\n",
    "windows(7,7)\n",
    "par(mfrow=c(2,2))\n",
    "plot(model)\n",
    "# interpretation of plots\n",
    "# plot 1: you do not want to see lots of structure or patern in the plot!\n",
    "# plot 2: shows the qqnorm-plot gives straight line for normally distributed errors\n",
    "# plot 3: identical to plot 1 on other scale\n",
    "\n",
    "+++++++++++++++++++++\n",
    "\n",
    "R-codes without comments:\n",
    "\n",
    "X<-matrix(c(0,1,2,3,4,5,6,7,8))\n",
    "Y<-matrix(c(12,10,8,11,6,7,2,3,3))\n",
    "plot(X,Y)\n",
    "model<-lm(Y~X)\n",
    "Sxis<-sum((X-mean(X))^2)\n",
    "Sxiyi<-sum((X-mean(X))*(Y-mean(Y)))\n",
    "rc<-Sxiyi/Sxis\n",
    "intercept<-mean(Y)-rc*mean(X)\n",
    "abline(model,col=\"red\")\n",
    "yhat<-predict(model,X=X)\n",
    "join<-function(i)\n",
    "\tlines(c(X[i],X[i]),c(Y[i],yhat[i]),col=\"green\")\n",
    "sapply(1:20,join)\n",
    "bs<-seq(rc-0.5,rc+0.5,0.01)\n",
    "SSE<-function(i) sum((Y-intercept-bs[i]*X)^2)\n",
    "plot(bs,sapply(1:length(bs),SSE),type=\"l\",ylim=c(25,160),\n",
    "\txlab=\"slobe b\",ylab=\"sum of squared residuals\",col=\"blue\")\n",
    "SSX=sum((X-mean(X))^2)\n",
    "SSY=sum((Y-mean(Y))^2)\n",
    "SSXY=sum((X-mean(X))*(Y-mean(Y)))\n",
    "rc=SSXY/SSX\n",
    "intercept=mean(Y)-rc*mean(X)\n",
    "#check the results with LSM\n",
    "lm(Y~X)\n",
    "deviance(lm(Y~X))\n",
    "SSR<-SSXY^2/SSX\n",
    "SSE<-SSY-SSR\n",
    "SSE1<-deviance(lm(Y~X))\n",
    "SSY1<-deviance(lm(Y~1))\n",
    "SSR1<-SSY1-SSE1\n",
    "anova(lm(Y~X))\n",
    "cv<-qf(0.95,1,7)\n",
    "1-pf(147.24,1,8)\n",
    "var<-(sum(Y^2)-intercept*sum(Y)-rc*sum(X*Y))/7\n",
    "seb<-sqrt(var/SSX)\n",
    "sea<-sqrt(var*sum(X^2)/(9*SSX))\n",
    "summary(lm(Y~X))\n",
    "\n",
    "+++++++++++++++++++++\n",
    "\n",
    "TW2\n",
    "\n",
    "MVS\n",
    "\n",
    "Reference:\n",
    "[1] Michael J. Crawley (2013). \"The R-book\". Chapter 10 Regression.\n",
    "\n",
    "R-code\n",
    "\n",
    "#10.6 Prediction following regression\n",
    "\n",
    "# predicting the future?\n",
    "# all branches of applied science rely upon prediction\n",
    "# main issue: how to deal with uncertainty?\n",
    "# uncertainty 1: suitability of fitted model\n",
    "# uncertainty 2: representativeness of the data used to parameterize the model\n",
    "# uncertainty 3: future conditions\n",
    "# two kinds of prediction: interpolation (within measured range) & extrapolation (beyond measured range) \n",
    "X<-matrix(c(0,1,2,3,4,5,6,7,8))\n",
    "Y<-matrix(c(12,10,8,11,6,7,2,3,3))\n",
    "plot(X,Y)\n",
    "model<-lm(Y~X)\n",
    "abline(model,col=\"blue\")\n",
    "# get the slope\n",
    "coef(model)[2]\n",
    "# standard error of the slope\n",
    "summary(model)[[4]][4]\t\t#try how this works!\n",
    "\n",
    "+++++++++++++++++++++\n",
    "\n",
    "# now focus on uncertainty of parameter estimates..\n",
    "# how to plot the estimated slope plus and minus one standard error of the slope:\n",
    "se.lines<-function(model){\n",
    "\tb1<-coef(model)[2]+summary(model)[[4]][4]\n",
    "\tb2<-coef(model)[2]-summary(model)[[4]][4]\n",
    "\txm<-sapply(model[[12]][2],mean)\n",
    "\tym<-sapply(model[[12]][1],mean)\n",
    "\ta1<-ym-b1*xm\n",
    "\ta2<-ym-b2*xm\n",
    "\t\tabline(a1,b1,lty=2,col=\"blue\")\n",
    "\t\tabline(a2,b2,lty=2,col=\"blue\")\n",
    "\t}\n",
    "\tse.lines(model)\n",
    "# Q: how to interpret the dotted lines?\n",
    "\n",
    "+++++++++++++++++++++\n",
    "\n",
    "# now focus on the uncertainty about predicted values..\n",
    "# how to draw the 95% confidence interval associated with predictions of Y at different intervals of x\n",
    "ci.lines<-function(model){\n",
    "\txm<-sapply(model[[12]][2],mean)\n",
    "\tn<-sapply(model[[12]][2],length)\n",
    "\tssx<-sum(model[[12]][2]^2)-sum(model[[12]][2])^2/n\n",
    "\ts.t<-qt(0.975,(n-2))\n",
    "xv<-seq(min(model[[12]][2]),max(model[[12]][2]),length=100)\n",
    "yv<-coef(model)[1]+coef(model)[2]*xv\n",
    "se<-sqrt(summary(model)[[6]]^2*(1/n+(xv-xm)^2/ssx))\n",
    "ci<-s.t*se\n",
    "\tuyv<-yv+ci\n",
    "\tlyv<-yv-ci\n",
    "\tlines(xv,uyv,lty=2,col=\"blue\")\n",
    "  \tlines(xv,lyv,lty=2,col=\"blue\")\n",
    "}\n",
    "# replot linear regression with confidence intervals\n",
    "plot(X,Y,pch=21,col=\"blue\",bg=\"red\")\n",
    "abline(model, col=\"blue\")\n",
    "ci.lines(model)\n",
    "\n",
    "+++++++++++++++++++++\n",
    "\n",
    "# QAD\n",
    "plot(X,Y,pch=16,ylim=c(0,15))\n",
    "model<-lm(Y~X)\n",
    "xv<-seq(0,8,0.1)\n",
    "yv<-predict(model,list(X=xv),int=\"c\")\n",
    "matlines(xv,yv,lty=c(1,2,2),col=\"black\")\n",
    "\n",
    "+++++++++++++++++++++\n",
    "\n",
    "TW2\n",
    "\n",
    "MVS\n",
    "\n",
    "Reference: \n",
    "[1] Johnson & Wichern (2002). \"Applied Multivariate Statistical Analysis\".\n",
    "[2] https://data.library.virginia.edu/diagnostic-plots/\n",
    "[3] https://data.library.virginia.edu/understanding-q-q-plots/\n",
    "\n",
    "# [1] 7.6 Model checking (..)\n",
    "\n",
    "# if the model is valid, the residual error is assumed to be a normal random variable with a mean of zero\n",
    "# cov matrix is not diagonal with cov=var*[I-H]\n",
    "# residuals have unequal variances and nonzero correlations\n",
    "# practice: correlations are small and vraiances are nearly equal\n",
    "# residuals should be plotted in various ways to detect possible anomalies\n",
    "# data\n",
    "# import the twosample.txt\n",
    "x<-twosample[,\"x\"]\t# extract the x variabe\n",
    "y<-twosample[,\"y\"]\n",
    "data<-cbind(x,y)\n",
    "model<-lm(y~x)\n",
    "# par(mfrow=c(2,2))\t# Change the panel layout to 2 x 2\n",
    "# par(mfrow=c(1,1)) \t# Change back to 1 x 1\n",
    "plot(model)\n",
    "\n",
    "# interpretation of the plots:\n",
    "# 1. residual versus fitted: shows if residuals have non-linear patterns.\n",
    "# look for equally sread residuals around a horizontal line indicationg NO non linear patterns\n",
    "# showing the dependence of the residuals on the predicted value. perfect is a horizontal band(width) indicating equal variances and no dependence on the predicted value of y (yhat)\n",
    "\n",
    "# 2. QQ plot: are the errors normally distributed?\n",
    "# a straight line indicates that residuals are normally distributed\n",
    "# some calculations by hand first iot calculate the error vector\n",
    "n<-length(y)\n",
    "x0<-matrix(1L,nrow=n,ncol=1)\n",
    "# terms in LSM\n",
    "Z<-cbind(x0,x)\n",
    "library(MASS)\n",
    "b<-ginv(t(Z)%*%Z)%*%t(Z)%*%y\n",
    "yhat<-Z%*%b\n",
    "yhat\n",
    "error<-y-yhat\n",
    "# QQ plot\n",
    "qqnorm(error)\n",
    "qqline(error)\n",
    "\n",
    "# 3. Scale-location \n",
    "# also called the spread-location plot\n",
    "# shows if residuals are spread equally along the line of predictors\n",
    "# good to see a horizontal line with equally spread points\n",
    "\n",
    "# 4. Residuals versus Leverage (hefboomwerking)\n",
    "# this plots helps to find influential cases (if any)\n",
    "# influence of outliers on the regression line\n",
    "# look for outlying values athe the upper or lower right corner (where cases are influential on the regline) outside the dashed lines (Cook's distance)\n",
    "# exclude cases from regression\n",
    "# pattern is NOT relevant\n",
    "# departures from the regression model are often hidden by the fitting process\n",
    "# outliers can have considerable effect but are hard to detect from the residual plot(s)\n",
    "# the average leverage equals h=(r+1)/n\n",
    "# change in yhat = h * (change in y)\n",
    "# leverage equals 1 means prediction is almost solely determined by that variable\n",
    "\n",
    "+++++++++++++++++++++\n",
    "\n",
    "TW2\n",
    "\n",
    "MVS\n",
    "\n",
    "Reference: \n",
    "[1] Johnson & Wichern (2002). \"Applied Multivariate Statistical Analysis\".\n",
    "\n",
    "# [1] 7.4 Inferences about the regression model\n",
    "## variables definition\n",
    "# z1~total dwelling size\n",
    "# z2~assessed value\n",
    "# Y~selling price\n",
    "z1<-matrix(c(15.31,15.2,16.25,14.33,14.57,17.33,14.48,14.91,15.25,13.89,15.18,14.44,14.87,18.63,15.2,25.76,19.05,15.37,18.06,16.35))\n",
    "z2<-matrix(c(57.3,63.8,65.4,57,63.8,63.2,60.2,57.7,56.4,55.6,62.6,63.4,60.2,67.2,57.1,89.6,68.6,60.1,66.3,65.8))\n",
    "Y<-matrix(c(74.8,74,72.9,70,74.9,76,72,73.5,74.5,73.5,71.5,71,78.9,86.5,68,102,84,69,88,76))\n",
    "data<-cbind(z1,z2,Y)\n",
    "n=length(Y)\n",
    "## design matrix Z\n",
    "z0<-matrix(1L,nrow=20,ncol=1)\n",
    "Z<-cbind(z0,z1,z2)\n",
    "r<-3\t#rank of Z\n",
    "## coefficients\n",
    "b<-ginv(t(Z)%*%Z)%*%t(Z)%*%Y\n",
    "model<-lm(Y~z1+z2)\n",
    "summary(model)\n",
    "# intercept=30.966, b1=2.634 and b2=0.045\n",
    "## error of the model\n",
    "e<-Y-Z%*%b\n",
    "summary(e)\n",
    "# assumption is that the errors are normally distributed: see 7.6 model checking\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lineaire regressie en de storingsterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data invoeren\n",
    "X<-c(12,2,3,5,10,9,8)\n",
    "Y<-c(125,30,43,62,108,102,90)\n",
    "n<-length(X)\n",
    "one<-rep(1,7)\n",
    "# matrix vergelijking\n",
    "A<-cbind(X,one)\n",
    "b<-Y\n",
    "# oplossing geeft vector met coefficienten a en b\n",
    "oplossing<-(solve(t(A) %*% A) %*% t(A) %*% b)\n",
    "# storingsterm (fout) bepalen\n",
    "error<-A %*% oplossing-b\n",
    "# deze foutenvector geeft per datapuntje het verschil met de best passende lijn\n",
    "# de variantie van de stroingsterm wordt geschat door de grootheid S[error]^2 (SE2), formule zie Buijs [1] p.382.\n",
    "SE2<-(t(error) %*% error)/(n-2)\n",
    "STDV2=sqrt(SE2)\n",
    "# Q: wat is de interpretatie van dit getal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td> 2.4404762</td></tr>\n",
       "\t<tr><td> 2.5595238</td></tr>\n",
       "\t<tr><td>-0.9523810</td></tr>\n",
       "\t<tr><td>-0.9761905</td></tr>\n",
       "\t<tr><td> 0.4642857</td></tr>\n",
       "\t<tr><td>-3.0238095</td></tr>\n",
       "\t<tr><td>-0.5119048</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{l}\n",
       "\t  2.4404762\\\\\n",
       "\t  2.5595238\\\\\n",
       "\t -0.9523810\\\\\n",
       "\t -0.9761905\\\\\n",
       "\t  0.4642857\\\\\n",
       "\t -3.0238095\\\\\n",
       "\t -0.5119048\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "|  2.4404762 | \n",
       "|  2.5595238 | \n",
       "| -0.9523810 | \n",
       "| -0.9761905 | \n",
       "|  0.4642857 | \n",
       "| -3.0238095 | \n",
       "| -0.5119048 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "     [,1]      \n",
       "[1,]  2.4404762\n",
       "[2,]  2.5595238\n",
       "[3,] -0.9523810\n",
       "[4,] -0.9761905\n",
       "[5,]  0.4642857\n",
       "[6,] -3.0238095\n",
       "[7,] -0.5119048"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De waarde van `SE2` is de variantie van de storingsterm. Dit is interessant omdat je daarmee kan laten zien hoe goed de passing van de best passende lijn is. Hoe lager, hoe beter de passing. We delen door $n-2$ omdat we twee variabelen hebben. Dit is $n$ minus het aantal vrijheidsgraden. Het aantal vrijheidsgraden is in dit geval $2$ omdat er twee variabelen zijn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td>4.797619</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{l}\n",
       "\t 4.797619\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| 4.797619 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "     [,1]    \n",
       "[1,] 4.797619"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SE2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De waarde van `STDV2` is de wortel van `SE2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td>2.190347</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{l}\n",
       "\t 2.190347\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| 2.190347 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "     [,1]    \n",
       "[1,] 2.190347"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "STDV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>X</th><th scope=col>one</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>X</th><td> 0.01190476</td><td>-0.08333333</td></tr>\n",
       "\t<tr><th scope=row>one</th><td>-0.08333333</td><td> 0.72619048</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & X & one\\\\\n",
       "\\hline\n",
       "\tX &  0.01190476 & -0.08333333\\\\\n",
       "\tone & -0.08333333 &  0.72619048\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | X | one | \n",
       "|---|---|\n",
       "| X |  0.01190476 | -0.08333333 | \n",
       "| one | -0.08333333 |  0.72619048 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "    X           one        \n",
       "X    0.01190476 -0.08333333\n",
       "one -0.08333333  0.72619048"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "solve(t(A) %*% A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alle bovengenoemde informatie is ook te vinden met het `summary(...)` commando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = X ~ Y)\n",
       "\n",
       "Residuals:\n",
       "       1        2        3        4        5        6        7 \n",
       " 0.27221  0.25310 -0.11271 -0.10888  0.05827 -0.31136 -0.05062 \n",
       "\n",
       "Coefficients:\n",
       "             Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept) -1.404956   0.228928  -6.137  0.00167 ** \n",
       "Y            0.105062   0.002646  39.701 1.91e-07 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 0.2305 on 5 degrees of freedom\n",
       "Multiple R-squared:  0.9968,\tAdjusted R-squared:  0.9962 \n",
       "F-statistic:  1576 on 1 and 5 DF,  p-value: 1.911e-07\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(lm(X~Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voor een regressiemodel willen we de waarden $a, b, \\sigma^2$ bepalen. Let er op dat in de US de vergelijking $y=a+bx$ is en in de EU $y=ax+b$.\n",
    "\n",
    "$$ \\epsilon \\rightarrow E(\\epsilon) = 0 \\land Var(\\epsilon)=\\sigma^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ S^2_\\epsilon = \\frac{\\sum e_i^2}{n-2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De term $\\sum e_i^2$ kan gemakkelijk worden berekend de vector van $e$, dus $e^T \\cdot e$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Het idee is dat voor een $x_0$ we een $y$ bepalen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ E(Y\\ |\\ x_0) = \\alpha + \\beta \\cdot x_0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ s_\\mu = s_\\epsilon \\cdot \\sqrt{ \\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{\\sum(x_i - \\bar{x})^2} } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naar mate je verder van het gemiddelde afkomt, wordt de fout groter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Betrouwbaarheidsinterval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Met de onderstaande formule kunnen we een betrouwbaarheidsinterval opstellen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y^c - t \\cdot s_\\mu < \\mu_y < y^c + t\\cdot s_\\mu $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "waarbij $t$ de t-verdeling is. Hiervoor hebben we $\\alpha=0.05$ en het aantal vrijheidsgraden nodig. Vervolgens kunnen we de $t$-waarde bepalen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
